# -*- coding: utf-8 -*-
"""ass81.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sFffFMiE7gjj00G3Hwep5achQuCmHqgq
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import scale

# %matplotlib inline

df = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')
print(df.head())
print(df.tail())

df.describe()

df.dtypes

"""[Attrition, BusinessTravel, Department, EducationField, Gender, JobRole, MaritalStatus, Over18, OverTime] are categorical variables.

DATA PREPROCESSING
"""

df.isnull().sum()

"""It tells that like there are no missing values in any of the variables in our dataset.

Before we proceed with find corelation among each and every variable lets drop those columns whose standard deviation is equal to 0 from df.describe()
"""

df.describe()

df = df.drop(["EmployeeCount","StandardHours","Over18","EmployeeNumber"],axis=1)

"""We calculate the corelation coefficient which gives us the strength among two varibles using the below function. Since Attrition is a varibale that has string values as variables I converted them to either 1 or 0 for calculation purposes."""

def corrcof(a,b):
    return np.array(np.cov(a,b)/ (np.std(a) * np.std(b)))
def convert(d):
        switcher = {
            'Yes': 1,
            'No':  0,
            'Married' : 2,
            'Single' : 1,
            'Divorced' : 0}
        return switcher.get(d)
df["Attrition"] = df["Attrition"].apply(convert)
df["OverTime"] = df["OverTime"].apply(convert)
df["MaritalStatus"] = df["MaritalStatus"].apply(convert)

df.head()

"""Now we check the corelation coefficient using corrcof() function declared above among each variable"""

x_attributes = [df.Age, df.WorkLifeBalance, df.DailyRate, df.DistanceFromHome, df.Education, df.EnvironmentSatisfaction, df.PerformanceRating, df.RelationshipSatisfaction, df.StockOptionLevel, df.TotalWorkingYears, df.TrainingTimesLastYear, df.WorkLifeBalance, df.YearsAtCompany, df.YearsInCurrentRole, df.YearsSinceLastPromotion, df.YearsWithCurrManager, df.OverTime, df.NumCompaniesWorked, df.PercentSalaryHike, df.MonthlyRate, df.MonthlyIncome, df.MaritalStatus, df.HourlyRate, df.JobInvolvement, df.JobLevel, df.JobSatisfaction]

def corr_each(a,b):
    j = 0
    for i in b:
        print()
        print(f"{a.name} and {b[j].name} " + f"({corrcof(a,i)[0,1]})")
        j += 1

corr_each(df.Attrition, x_attributes)

"""we can use Logistic Regression

From the above corelation coefficient values we can say that Employee Attrition is likely to be related on [JobSatisfaction, JobLevel, JobInvolvement, MonthlyIncome, Overtime, YearswithCurrManager, YearsInCurrentRole, YearsAtCompany, TotalWorkingYears, StockOptionLevel, EnvironmentSatisfaction, Age]
"""

df.Attrition.value_counts()/1470*100

df.groupby("Attrition").mean()

pd.crosstab(df.Attrition, df.OverTime).plot.bar()
pd.crosstab(df.Attrition, df.EnvironmentSatisfaction).plot.bar()
pd.crosstab(df.Attrition, df.JobSatisfaction).plot.bar()
pd.crosstab(df.Attrition, df.JobInvolvement).plot.bar()

pd.crosstab(df.Attrition,df.StockOptionLevel).plot.bar()
pd.crosstab(df.Attrition, df.JobLevel).plot.bar()

x_attr = ['JobLevel','StockOptionLevel','JobSatisfaction','EnvironmentSatisfaction','OverTime','Age','TotalWorkingYears','YearsAtCompany','MonthlyIncome']
df_y = df['Attrition']
df_x = df[x_attr]

X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.75, random_state=42)
#X_train = scale(X_train[['MonthlyIncome','Age','TotalWorkingYears','YearsAtCompany']])
#X_test = scale(X_test[['MonthlyIncome','Age','TotalWorkingYears','YearsAtCompany']])
logr = LogisticRegression(max_iter=1000)
logr.fit(X_train,y_train)
y_pred = logr.predict(X_test)
accuracy_score(y_test,y_pred)

confusion_matrix(y_test, y_pred)
p = pd.DataFrame(y_pred)
p[0].value_counts()

logr.score(X_test, y_test)*100

print(classification_report(y_test, y_pred))

acc1 = cross_val_score(logr,df_x, df_y, cv=10, scoring='accuracy') #Cross Validation to check if our model is prone to Overfitting.
acc1.mean()*100

"""After implementing the Logistic Regression, the score obtained is 85.76% for this model and this dataset. From the confusion matrix we can say that there are 930 correct predictions and 173 wrong predictions. Will implement the k-NN Algorithm on the same dataset and check with the results.

KNN MODEL
"""

knn = KNeighborsClassifier(n_neighbors = 10,metric ='euclidean')
X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.75, random_state=42)
knn.fit(X_train, y_train)
y_predi = knn.predict(X_test)
confusion_matrix(y_test,y_predi)
knn.score(X_test, y_test)

plt.scatter(X_test['OverTime'],X_test['MonthlyIncome'],c=y_predi, cmap = 'coolwarm')

df_y.value_counts()

print(classification_report(y_test,y_predi))

acc2 = cross_val_score(knn, df_x, df_y, scoring='accuracy', cv=10) #Cross Validation to check if our model is prone to Overfitting.
acc2.mean()*100

"""We can see that the accuracy level of logistic regressor is high than a kNN classifier. This lazy learning algorithm is given a k-value of 10 which takes 10 nearest neighbors in each fold. The claasification report which give us the details such as F-Measure, Precision, Recall and both the models are provided with such report.

HYPER PARAMETER TUNING
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.utils import resample
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# %matplotlib inline

df = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')

df.head()

"""IMPORT MODELS"""

def logi_reg(x_var, y_var, t_size) :
    X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size = t_size, random_state =2)
    lr = LinearRegression(max_iter = 1000).fit(X_train,y_train)
    y_pred = lr.predict(X_test)
    score = accuracy_score(y_pred, y_test)      #This gives us the accuracy score of these predictions
    c_report = classification_report(y_test, y_pred)      #This report displays the F1-Measure, Precision, Recall
    
    return lr

def knn_class(x_var, y_var, t_size, k):
    X_train, X_test, y_train, y_test = train_test_split(x_var, y_var, test_size = t_size, random_state = 2)
    knn = KNeighborsClassifier(n_neighbours = k, metric = 'euclidean').fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    score = accuracy_score(y_pred, y_test)             #This gives us the accuracy score of these predictions
    c_report = classification_report(y_test, y_pred)   #This report displays the F1-Measure, Precision, Recall
    
    return knn

"""Observations made from this dataset and impressions on the performance of both the models"""

df["Attrition"].value_counts()/1470*100

print(classification_report(y_test,y_predi))  #kNN Classifier

print(classification_report(y_test,y_pred)) #LogisticRegression

"""As we can see from the above frequency distribution of the class variable we can declare that this dataset is purely imbalanced. And also the recall which signifies how true the calues are in a given prediction set is low to the '1' class variables. To tackle this I have made an implementation by oversampling the minority class  which also improves the accuracy and recall of our minority variable."""

#data preprocessing over sampling(minority class)
X = pd.concat([X_train,y_train],axis=1)
X

y_a = X[X.Attrition == 1]
n_a = X[X.Attrition == 0]

# Commented out IPython magic to ensure Python compatibility.
from sklearn.utils import resample
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# %matplotlib inline
y_a_s = resample(y_a, replace=True, n_samples=len(n_a), random_state=42)
df_s = pd.concat([y_a_s, n_a])
df_s_x = df_s.drop(['Attrition'], axis=1)
df_s_y = df_s.Attrition
df_s['Attrition'].value_counts()/1724*100

df_x = df[['JobLevel','StockOptionLevel','JobSatisfaction','EnvironmentSatisfaction','OverTime','Age','TotalWorkingYears','YearsAtCompany','MonthlyIncome']]
df_y = df.Attrition

X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.75, random_state =42 )

#data preprocessing(Oversampling minority class)
X = pd.concat([X_train,y_train],axis=1)
X

y_a = X[X.Attrition == 1]
n_a = X[X.Attrition == 0]

y_a_s = resample(y_a, replace=True, n_samples=len(n_a), random_state=0)
df_s = pd.concat([y_a_s, n_a])
df_s_x = df_s.drop(['Attrition'], axis=1)
df_s_y = df_s.Attrition
df_s['Attrition'].value_counts()/1724*100

X_train, X_test,y_train, y_test = train_test_split(df_x,df_y, test_size=0.75, random_state=42)
log_reg = LogisticRegression(max_iter = 1000).fit(X_train, y_train)
y_pred = log_reg.predict(X_test)
accuracy_score(y_pred, y_test)
print(classification_report(y_test, y_pred))

cvs = cross_val_score(log_reg,df_s_x, df_s_y, cv=10, scoring='accuracy')
cvs.mean()*100

knn = KNeighborsClassifier().fit(X_train, y_train)
y_predi = knn.predict(X_test)
accuracy_score(y_predi, y_test)*100

print(classification_report(y_test, y_predi))

cvs_knn = cross_val_score(knn, df_s_x, df_s_y, scoring ='accuracy', cv=10)
cvs_knn.mean()*100

plt.scatter(X_test['OverTime'], X_test['MonthlyIncome'], c=y_predi, cmap='coolwarm')

p = np.logspace(-5,8,18)
param_grid = {"C" : p}
logreg_CV = GridSearchCV(log_reg, param_grid, cv=5)
logreg_CV.fit(X_train, y_train)
logreg_CV.best_params

logreg_CV.best_score_*100

paramgrid={
      'n_neighbors' : [3,5,11,19],
    'weights' : ['uniform','distance'],
    'metric' :['euclidean','manhattan']
}
knn_CV = GridSearchCV(knn, paramgrid, cv=3, verbose=1, n_jobs = -1)
knn_CV.fit(X_train, y_train)
knn_CV.best_params_

knn_CV.best_score_*100

#hyper parameter tuning(Randomized search cv)
logreg_CV2 = RandomizedSearchCV(log_reg, param_grid, cv=5)
logreg_CV2.fit(X_train, y_train)
logreg_CV2.best_params_
logreg_CV2.best_estimator_

logreg_CV2.best_score_*100

knn_CV2 = RandomizedSearchCV(knn, paramgrid, cv=3, verbose=1, n_jobs = -1)
knn_CV2.fit(X_train, y_train)
knn_CV2.best_params_

knn_CV2.best_score_*100